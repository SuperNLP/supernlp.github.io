---
layout: post
title: EMNLP 2018 Thoughts and Notes
---

### This year’s EMNLP was bigger than any ACL conference ever before. With around 2500 attendees, a couple of hundred talks, and a few hundred posters, it was unthinkable to try and keep track of everything. Nevertheless, in this day-after-the-conference blog, bubbling over with ideas, we will do just that. Starting from the first workshop and concluding with Johan Bos’ _Moment of Meaning_, we will attempt to offer a convincing account of what happened in Brussels and what it meant for the future of NLP.


# WNUT 2018

## Using Wikipedia edits in low resource grammatical error correction
### This paper builds a grammatical error correction (GEC) system for German. The authors combine two previously available German learner corpora: Falko and MERLIN to form the new Falko-MERLIN GEC corpus which contain ~24k sentences and their corrections. Next, they use the German Wikipedia edits to create a noisy, but bigger dataset. The ERRANT tool is used to filter this data by using the Falko-MERLIN corpus as the gold standard. A part of the Falko-MERLIN corpus is set aside as development and test sets and the rest is combined with Wikipedia data to train a convolutional seq2seq network to correct grammatical errors. The results show that incorporating the noisy Wikipedia data increases the accuracy of corrections. Including only the filtered part of Wikipedia increases it a bit more.

# Deep Latent Variable Models of Natural Language 
![alt text](
        {{supernlp.github.io}}/assets/img/vae.png  
      )
      
### In this tutorial, Alexander Rush, Yoon Kim, and Sam Wiseman offered a rather dense coverage of a class of models which is becoming more and more widely used in NLP. Latent variable models have been around for a long time in statistical modelling (see: Factor analysis, LISREL, etc.). A latent variable is one which is not directly observed but which is assumed to affect observed variables. Latent variable models therefore attempt to model the underlying structure of the observed variables, offering an explanation of the dependencies between observed variables which are then seen as conditionally independent, given the latent variable(s). 

### The tutorial starts by introducing the types of latent variable models (discrete, continuous, and structured), their connection to deep learning models, and example NLP applications. It then proceeds to explain ELBO, the learning objective commonly used for such models. Inference strategies (exact, sampling, and conjugacy) are then examined before they delve into more advanced topics such as the Gumbel-Softmax. Finally, three recent deep latent variable NLP models and the tricks used to make them work are treated. Overall, this is a great resource for a direction of research that is certain to grow:

### [https://nlp.seas.harvard.edu/latent-nlp-tutorial.html](https://nlp.seas.harvard.edu/latent-nlp-tutorial.html)

# CONLL 2018
## [Sequence Classification with Human Attention](http://aclweb.org/anthology/K18-1030)

### **_Full disclosure:_** this paper comes from our own group, so we might be a little biased here. Speaking of which, a recent theme in the field has been the call for more inductive bias and ways to introduce it. In this paper, the authors connect dots from previous work in our group by combining techniques known from multi-task learning with signals of human language processing. In particular, they use a bidirectional RNN with attention for sequence classification. So far so good, but the interesting part is that the network’s attention mechanism is enticed to more closely model human attention as attested by gaze patterns recorded in eye-tracking experiments. This is achieved through an auxiliary loss which, notably, can be minimized from external gaze data, such that no eye-tracking recordings are needed at test time. The paper shows that across three sentence classification tasks, biasing the machine attention towards human attention leads to consistent performance gains and results in sensible attention patterns that both validate the approach and help analyzing system decisions. 

![alt text](
        {{supernlp.github.io}}/assets/img/award.jpeg  
      )

# Staring Into the Darkness
![alt text](
        {{supernlp.github.io}}/assets/img/blackbox.png 
      )
      
### Blackbox NLP was one of the most highly anticipated workshops at the conference (not just for me, apparently, 700 registrants!) and it delivered on its promise. The workshop dealt with the problem of interpretability in neural network models: how can we know what is encoded in representations learned by our models? What are they capable of learning and what are they not capable of learning? Inspired by previous work on [representation analysis](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2605405/) and on examining the [linguistic generalisation learned by neural models](https://arxiv.org/abs/1611.01368), the workshop offered a way forward for dealing with a problem which has long been seen as an elephant in the proverbial NLP room. 

### It kicked off with a talk by __Yoav Goldberg__ which stressed the importance of the recent trend of work which treats NLP models/representations as “Organisms” and constructs hypothesis which are then tested empirically - an approach which is uncommon in computer science. He then described ongoing work on testing the expressive and computational capabilities of and extracting discrete representations from RNNs: [http://proceedings.mlr.press/v80/weiss18a.html](http://proceedings.mlr.press/v80/weiss18a.html), [https://arxiv.org/abs/1808.09357](https://arxiv.org/abs/1808.09357), [https://arxiv.org/abs/1805.04908](https://arxiv.org/abs/1805.04908). 

### Slides: [http://u.cs.biu.ac.il/~yogo/blackbox2018.pdf](http://u.cs.biu.ac.il/~yogo/blackbox2018.pdf)

### In the next talk, it was __Graham Neubig’s__ turn to make the case for incorporating linguistic structure in neural models through latent variable modelling (sound familiar? See the first section in this blog!). Specifically, in the talk Graham describes three recent models:

#### 1. [Multi-space Variational Encoder-Decoder](https://arxiv.org/abs/1704.01691) for labeled sequence transduction with semi-supervised learning which models the meaning of a word (a complex, high-level concept) as a continuous variable (using the reparametrization trick) and the closed-class and interpretable morphological features as discrete variables (using the Gumbel-Softmax). The model performs supervised learning by maximizing the variational lower bound on the marginal log likelihood of the data and annotated labels. For unlabeled data, an autoencoding objective is used to construct a word conditioned on its lemma and the set of discrete latent variables (a proxy for the morphological labels) which is associated with a it. 

#### 2. [StructVAE](https://arxiv.org/abs/1806.07832) which again performs semi-supervised learning (with structured latent variables) by employing an off-the-shelf semantic parser to parse natural language into latent meaning representations and a decoder to reconstruct the meaning representations into natural language. This allows both supervised training of the inference model on parallel Natural Language-Meaning representation data, and unsupervised training on natural language alone by maximizing the variational lower bound of the likelihood using REINFORCE. 

#### 3. [Unsupervised Learning of Syntactic Structure with Invertible Neural Projections](https://arxiv.org/abs/1808.09111) which uses invertible neural projection (super nifty, ha?) to enable the learning of word embeddings which are conditioned on the latent syntactic representation (e.g. dependency parse) in an unsupervised setting. The usefulness of this is demonstrated for both Unsupervised POS tagging (which is modelled with a markov prior) and unsupervised dependency parsing (DMV). 

### Slides: [http://www.phontron.com/slides/neubig18blackbox.pdf](http://www.phontron.com/slides/neubig18blackbox.pdf)

### Finally it was Leila Wehbe’s turn to awe the predominantly NLP-oriented audience with a fresh perspective from the intersection of machine learning and cognitive science. She presented very exciting work about how the representations from NLP’s various artificial neural network models can be used to analyse complex brain activity (MEG and fMRI) data. Among the findings she presented it was:


#### 1. MEG activity (MEG can resolve events with a precision of 10 milliseconds or faster) is well predicted by word embeddings, but poorly predicted by (universal) sentence encoders.

#### 2.  For fMRI data (which has a latency of several hundred milliseconds), both word embeddings and sentence encoders can be predictive of brain activity. Here is the catch, though: they predict different regions of the brain! Put very roughly, word embeddings are more predictive of regions associated with initial semantic processing, while sentence encoders (which include context + current word or just context) better predict regions associated with later semantic processing and composition. 

#### The talk concludes on a speculative note and with a reference to something I’ve been running into a lot lately, David Marr’s levels of analysis (http://blog.shakirm.com/2013/04/marrs-levels-of-analysis/), which I’ll link to but not further discuss in this blog. Food for thought. 

### Other Workshop Highlights:

### + [Interpretable Structure Induction Via Sparse Attention](http://aclweb.org/anthology/W18-5450): _Can dynamic sparse, regularized, constrained and structured tells our models to attend to produce a decision?_
### + [Understanding Convolutional Neural Networks for Text Classification](https://arxiv.org/pdf/1809.08037.pdf): _What does each filter capture? Which n-grams contribute to the classification?_
### [Extracting Syntactic Trees from Transformer Encoder Self-Attentions](http://aclweb.org/anthology/W18-5444): _Self-attention weights from MT systems are extracted to build trees? Where is the attention mostly directed? Does it resemble trees from linguistic theories?_
### __And the best paper:__ [Under the Hood: Using Diagnostic Classifiers to Investigate and Improve how Language Models Track Agreement Information](http://aclweb.org/anthology/W18-5426): _Using diagnostic classifiers to both understand and improve neural models._


# Main Conference: Day One Highlights

### [Cross-lingual Knowledge Graph Alignment via Graph Convolutional Networks](https://aclanthology.info/papers/D18-1032/d18-1032)
#### This paper proposes a new approach to KB alignment using graph convolutional networks (GCN). They leverage entity embeddings learned by the GCNs, using both relational and the attributional triples in KGs to learn entity alignments. Their approach shows good improvements over previous methods and seems like a step in the right direction. 

### [Neural cross-lingual named entity recognition with minimal resources](https://arxiv.org/abs/1808.09861)
#### This paper attempts to tackle NER in an unsupervised transfer setting. The assumption is that there is labelled data in the source language, monolingual corpora in both source and target languages and a small dictionary of words between the. They use English (source language) to learn NER and try to transfer it to Spanish, German and Dutch (target languages). The method is as follows: First, they train monolingual word embeddings and project them into a shared space using the dictionary words as anchor points. This alignment is refined iteratively by inducing new/better dictionaries using source and target words which are very close to each other in the shared space. They then translate source language’s dataset by treating each word’s nearest neighbor as its translation and train an NER on this translated dataset. The model used is a hierarchical Bi-LSTM with self-attention and CRF.

### [Learning to split and rephrase from Wikipedia edit history](https://arxiv.org/abs/1808.09468)
#### This paper releases a new dataset for text simplification called [WikiSplit](https://github.com/google-research-datasets/wiki-split) which contains one million data points in English. The approach taken to create the dataset is to look at adjacent versions of Wikipedia articles and identify candidate pairs using tri-gram affixes. These are then filtered for sentence similarity using BLEU. This dataset has about 33.1 million running tokens compared to 344k tokens of WebSplit, which was previously the biggest such dataset. They use a one-layer LSTM with attention and copying and train it both on WebSplit and WikiSplit. They observe a ~30 point increase in BLEU when the model is trained on WikiSplit. The the datasets are combined, the increase in BLEU is ~32 points.

### Textual Analogy Parsing: What’s Shared and What’s Compared among Analogous Facts
#### This paper focuses on a particular discourse phenomenon, textual analogy, which the authors define as a higher-order semantic relation that captures the points of similarity and difference between semantic role structures. They define the new task of Textual Analogy Parsing (TAP), the goal of which is to extract analogy frames from text. They release a new dataset of expert-annotated quantitative TAP frames from the WSJ corpus, and show that once extracted, quantitative TAP frames can be visualized as plots. Thus, TAP forms the basis for the new application of automatically plotting quantitative text. Their best model combines ideas from recent neural approaches to SRL and co-ref, but interestingly benefits a lot from using integer linear programs (ILPs) to decode into structured outputs. Using thorough error analysis, authors suggest that there is still room for improvement on the TAP task.



