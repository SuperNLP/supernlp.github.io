---
layout: post
title: Sent rep
comments: true
description: This year’s EMNLP was bigger than any ACL conference ever before. With around 2500 attendees, a couple of hundred talks, and a few hundred posters, it was unthinkable to try and keep track of everything. Nevertheless, in this day-after-the-conference blog, bubbling over with ideas, we will do just that. Starting from the first workshop and concluding with Johan Bos’ Moment of Meaning, we will attempt to offer a convincing account of what happened in Brussels and what it meant for the future of NLP.
---


# I. A problem of compositionality?

The <span class="tooltip" data-tippy="I'm a tooltip!">problem</span> of finding ways to go from vectorial word representations to representations of longer pieces of text (such as phrases, sentences, paragraphs, etc.) is one which has been an active area of research for several years, with recent developments showing more and more promise. At the crux of the issue lies the fundamental idea of compositionality in vector space models. This is can be loosely described as the ability to compose parts (e.g. words) into something larger (e.g. sentences) and to analyse the larger units as combinations of constituents. Now this is in itself  a fascinating topic of research which goes beyond NLP and Machine Learning and indeed has a rich history in physics and in social and cognitive sciences (see https://sites.google.com/view/capns2018/home).  

Within NLP, research into methods of compositionality kicked off once it became clear that word vector space models based on distributional information that capture semantic information could be constructed. The simplest method of building these word vector space models involves computing a matrix of co-occurrence for all words in a corpus and then applying some form of dimensionality reduction using matrix factorisation techniques (see Glove). 

A glaring complication with extending vector representations of words to sentences is the joint question of both what precisely a vector ought to represent, and how it ought to learn this. Clearly, naive word compositionality is not a valid technique for retaining meaning: similar words can be combined to form sentences with very different meanings.

### On supervision

The distinction between supervised and unsupervised learning is one most people are familiar with: supervised learning involves labelled data, whilst unsupervised learning involves unstructured data. Relevant to sentence representations, however, the term needs some extra clarification. Some background: it’s important to note, at this stage, that a large chunk of sentence representation papers deal with transfer learning. That is, the corpora the representations are trained on might have absolutely nothing to do with the corpora for the tasks they’re used in downstream. So from this point on, unless otherwise mentioned, the supervised/unsupervised dichotomy refers to just the way the representations were trained, and not to the tasks they were evaluated on.

# II. History
Drawing a line between the ‘history’ and the ‘present’ of sentence representation learning -- a new subdomain of a new discipline -- is always going to be fraught with arbitrariness. However, there has absolutely been a steep increase in the number of papers relevant to sentence representations being published every year. We therefore embrace the arbitrariness and attempt to summarise some foundational papers that we believe are ‘historical’ enough to warrant their own section.

In both this section and the next, we avoid discussing results: these are presented, with significant comment, in a later section.

MO -- Lapata

Le and Mikolov -- todo

Kiros et al. present a seminal approach to unsupervised sentence representation learning in Skip-Thought Vectors *CITE*. The idea is as ingenious as it is simple: they use an encoder-decoder architecture to attempt to predict, given a sentence, the next and previous sentences. The encoder and decoder, inspired by similar architectures in neural machine translation, use a GRU to represent the sentence, with the encoder’s final hidden state being used by the decoder to reconstruct neighbouring sentences.

Another widely cited work is Wieting et al.’s Paragram paper (2015). This one, unlike the others, is supervised -- representations are trained on phrase pairs from paraphrase data (PPDB). The try to force the representations of both phrases to be as close to each other as possible, and evaluate a bunch of different encoders. To their surprise (and to everyone without a GPU’s relief), their simplest model -- the average of word embeddings in a phrase -- works a lot better than the more complicated models that involve, for instance, variants of recurrent networks.
They also revisit this paper in 2016, where they try to use character representations to compose sentences; it performs better, and of course, they christen it Charagram.

Arora et al. 

Finally, we have Gan et al.’s unsupervised attempt to learn sentence representations using convolutional networks, which have been suspiciously AWOL thus far. They begin at the word level, using CNNs to encode words, and a more vanilla RNN to decode the next word *TODO*. Not content with sticking to words, they expand this to sentences, and ‘paragraphs’ (which are arbitrarily long chunks of text).

# III. Modern era
At this point, it makes a lot of organisational sense to divvy up the papers we’re going to be discussing, into two broad categories -- supervised and unsupervised.

### Supervised methods
Recent output in sentence representation learning involved a lot of interestingly leveraged datasets. The first such paper was Conneau et al.’s  work on natural language inference data. The task of natural language inference involves, given a premise and hypothesis sentence, classifying the relation of the hypothesis to the premise by describing whether it entails the premise, contradicts it, or does absolutely nothing relevant to it. The authors experimented with several architectures -- BiLSTMs with pooling, self-attentive networks and hierarchical ConvNets. They encoder both premises and hypotheses with these encoders, creating output vectors that they plug into a classifier: eventually, the BiLSTM with max pooling winds up being the most effective encoder.

Meanwhile, McCann et al. rely on a very different dataset: they use neural machine translation systems to train encoders, which they then use downstream. They combine these sentence-sensitive word representations (called CoVe) in a downstream network that includes biattention, to obtain sentence representations relevant to the downstream tasks. So, very technically, they aren’t really generating sentence embeddings -- they’ve just created a pre-trained framework that later uses downstream task information to generate embeddings. *img*

Next, we have Kiela et al. integrating information from images into sentence representation. The idea’s pretty straightforward -- they use the COCO dataset, which contains images and multiple (upto 5) captions per image, and evaluate three models on it. The first uses a series of projections to map image caption representations into the image’s ResNetted vector space. The second ignores the images, and tries to minimise the distance between the (multiple) captions per image, and the third combines the two approaches. They also augment their models with embeddings learnt without supervision (Skip-Thought), the intuition being that COCO ought to help with representations of concrete objects but less with abstract concepts.

Finally, there’s the more recent work by Subramanian et al., that hijacks the multi-task learning hype train. They combine the unsupervised Skip-Thought vectors with supervisedly learnt representations from a bunch of tasks: machine translation (En-De, En-Fr), constituency parsing and natural language inference. The original paper includes a detailed evaluation section with solid ablation testing.

Most of the supervised tasks we’ve described here involve supervised training on corpora that are often distinct from the downstream task they’ve been evaluated on. There’s still several approaches to sentence representation learning, though, that do not involve any sort of transfer learning: they attempt to condense sentence meaning specifically during the relevant task. This is a wee bit similar to the use of the final state of a recurrent network by a decoder in neural machine translation, but attempts to take sentence dynamics into account.

The first such paper we’ll talk about is on “sentence-state” LSTMs, which is an interesting take on the “let’s modify an LSTM” theme, which is in itself an interesting take on the “let’s modify an RNN” theme. *check* Basically, they extend the LSTM by representing every time-step (which is very often just a word) with word-states for all words in that sentence, along with an independent sentence state. Updates are then based on weighted combinations of trigram word-states at the previous time-step, the previous sentence state, and the current input. The first of these is a bit complicated, so in simpler language: updates use notions of how the previous word sees itself and its neighbours, how the previous word sees the whole sentence, and the current input. This is contrasted to how the regular forward LSTM only cares about how the previous state sees what precedes it, and its own input. It’s a cool idea, although it is a shame that they didn’t evaluate how it performs in transfer.

The next paper is also a bit of a modification of an LSTM. Lin et al. combine a standard LSTM with multi-headed self-attention, à la the famous Attention Is All You Need. Basically this involves first using a simple linear combination on the hidden states of a (bi)LSTM to generate multiple “mixes” along a particular hidden representation axis, which are then linearly combined yet again, after passing through a softmax normaliser. This is repeated multiple times to obtain multiple heads: our sentence representation is, therefore, a matrix and not a vector. The authors hypothesise that the use of attention takes some of the long-term memorisation load off the LSTM, as they have quick access to all timesteps.

Finally, to conclude this section, we review the very recent capsule network inspired Dynamic Self-Attention (Yoon et al.) *mostafa*

### Unsupervised methods
The first of our modern unsupervised representation learning approaches involves the use of the ByteLSTM (Radford et al.). Whilst the concept is fairly simple -- they use character-level LSTMs to model sentences -- what makes this paper really cool is their focus on domains. They break from tradition (all two years of it, anyway) a bit and use the Amazon product reviews corpus as their training data, because they want to focus on high-quality sentiment representation. Their results confirm the usefulness of domain, as they obtain massive improvements on movie and product reviews, whilst their results on other classification datasets aren’t particularly brilliant. The philosophy of evaluation is, however, an important component in this blog post, and we’re going to discuss it in more detail in a later section.

Next, we have the Universal Sentence Encoder, which is more of a use-oriented encoder release. The system (implemented in TensorFlow) includes two encoders -- the first is a transformer, which is basically the encoder bit from Attention Is All You Need, and the second is a ‘deep averaging network’ -- which, inspired by Iyyer et al., basically uses a deep network to simulate something akin to an average over the word representations in a sentence - the idea being that whilst averages can’t capture phenomena like words being replaced by their antonyms, letting deep magic do some magic might. Both networks are also augmented with supervised training, making this paper fit into both this section and the previous one.

Next, we take a look at Logeswaran and Lee’s efficient framework for learning sentence representations, in An Efficient Framework for Learning Sentence Representations. It’s pretty efficient -- they swap out the reconstruction style target-sentence-building of Skip-Thoughts with multiple choices. Essentially, given a sentence, the model learns to predict what the next sentence is, given a set of candidate sentences. The idea is that this can help empower the network to care less about surface forms (who needs words anyway?) and focus on meaning (not as deep as it sounds - it’s just a vector).

Our next paper is, relatively, a lot more mathematically motivated than the others we’ve reviewed. Tang and de Sa show us how  to exploit invertible decoders. The structure is the familiar unsupervised predict-a-sentence-given-the-previous-one. They build an encoder-decoder system that tries to maximise the averaged log-likelihood for every word in the target sentence, given the representation for that word, and the decoded last GRU hidden state for the source sentence. The catch is that their decoders are all invertible mathematical functions: the first of these is a linear projection *eqn*, that, when inverted gives us *eqn*. The second decoder is a bijective function, that creates a one-to-one mapping between the GRU hidden state, and word representations. This means, of course, that the two have to be exactly the same size, which is achieved by adding a linear transformation to normalise sizes. The test phase then involves merely inverting the decoder and using it to obtain sentence representations.

We conclude this section with Yang et al.’s Zero-Training Sentence Embeddings via Orthogonal Basis. As the name suggests, this paper isn’t even unsupervised -- it involves absolutely no training of its own, and instead relies on effectively combining pretrained word embeddings. *todo*

# IV. On evaluation
A pressing issue with comparing evaluations of all our systems is how differently many of them have been trained. First, there’s the fact that several approaches don’t involve any sort of transfer learning, despite being, in theory, capable of doing so. Next, particularly with the unsupervised approaches, some of the systems we’ve reviewed use different corpora. While one can say that that’s still perfectly fair and oughtn’t to stop anyone from comparing results, this shifts the evaluation from being an evaluation of the conceptual performance of encoders to being an evaluation of their conditional performance, conditioned by their environments (that include domains, corpora sizes, etc.). We have two goals in this sentence -- the first is to present and compare evaluation protocols specifically designed for sentence representations, and the second is to attempt to compare these results taking into account all of the many caveats, to the extent that they are comparable.

### Evaluation systems
In this section, we’ll take a look at several protocols, or packages, specifically catered towards evaluating sentence representations. All of these are extremely recent: none are pre-2018. We begin by discussing Conneau and Kiela’s SentEval toolkit: a toolkit that makes the process of evaluating sentence representations in transfer extremely easy. SentEval is, fundamentally, a collection of tasks, selected based on “what appears to be the community consensus”. The toolkit comes with several preprocessing scripts and a set of both supervised and unsupervised downstream tasks: the supervised tasks rely on fixing the sentence representations obtained prior to transfer and training the parameters of the classifier, whilst the unsupervised ones rely on eg. similarity metrics. SentEval is an extremely welcome utility, not just because of the ease it brings to representation evaluation, but also because of the standardisation it forces. The authors note that the non-transfer encoders they evaluated, which train representations on specific tasks, perform better than transfer-based methods, and point out that overcoming that might be an interesting research direction.

Similar in spirit to SentEval, but very distinct in methodology, is Conneau et al.’s paper on probing tasks. These are a set of tasks that, instead of measuring downstream performance of representations on concrete tasks, attempt to evaluate exactly how much information about the sentence itself representations can store. In a sense, they involve querying (“probing”)  a sentence representation for properties that ought to be visible given the original sentence. They probe for a total of ten phenomena over three categories: surface-level (eg. sentence length), syntax-level (eg. tree depth) and semantic-level (eg. tense). The authors also correlate performance on these probing tasks to downstream performance on SentEval tasks: surface-level phenomena wind up being significantly more strongly correlated than syntactic or semantic phenomena.

Two other recent evaluation systems deserve a quick mention despite not being as strictly relevant to the content of this post. The first of these is the GLUE benchmark (Wang et al.), a collection of natural language tasks specifically geared towards natural language understanding; thus, the selection of downstream tasks is one of the most significant differences between GLUE and SentEval, the other one being that GLUE, as a framework, does not evaluate just sentence representations in isolation, but entire systems: it therefore allows (encourages, even) the use of interaction (eg. soft attention) between input sentence pairs. The other such system is less of a system and more of a corpus -- the XNLI corpus (Conneau et al.) is a translation of the development and test portions of the older English-only MultiNLI corpus into 14 other languages, and is, as such, extremely useful in kickstarting research on multilingual sentence representations.

### Results
We shall now attempt to clear some of the fog when it comes to evaluating representations. This is a tricky subject for several reasons, because it involves comparing: i) supervised and unsupervised methods; ii) unsupervised methods trained on different corpora; and iii) transfer and non-transfer based methods. Another annoyance is the fact that most of the sentence representation techniques we’ve mentioned precede SentEval, and as such, have often been evaluated on different downstream tasks. Thankfully, there’s at least some overlap in what tasks these papers tended to select, which is more or less equivalent to the tasks included in SentEval, i.e.:
A set of classification tasks: these include MR and SST (movie reviews), CR (product reviews), SUBJ (subjectivity/objectivity), MPQA (opinion polarity), TREC (question type classification).
A variety of Natural Language Inference datasets, that involve specifying whether one of two statements entails the other: specifically, the SNLI and SICK-E corpora.
Semantic relatedness tasks, that involve ranking the semantic similarity between two sentences: the STS and SICK-R corpora.
Paraphrase identification, which involves identifying whether one of two sentences is a paraphrase of the other: this uses the MSRP dataset, also referred to as MRPC in some papers.
COCO, a dataset of images and corresponding captions; the relevant downstream task involves ranking captions by their relevance to given images, or vice versa.

This gives us our final results, arranged into sections where we judge comparisons as valid:


<div>
  <table>
    <tr>
      <th><span>Paper</span></th>
      <th><span>Trained on</span></th>
      <th><span>MR</span></th>
      <th><span>CR</span></th>
      <th><span>SUBJ</span></th>
      <th><span>MPQA</span></th>
      <th><span>TREC-6</span></th>
      <th><span>SST-2</span></th>
      <th><span>MSRP</span></th>
      <th><span>SNLI</span></th>
      <th><span>SICK-E</span></th>
      <th><span>SICK-R</span></th>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Conneau et al. (2017), <i>Supervised Learning of Universal Sentence Representations from Natural Language Inference Data</i>">InferSent</span>, BiLSTM-Max</td>
      <td><span>AllNLI</span></td>
      <td><span>81.1</span></td>
      <td><span>86.3</span></td>
      <td><span>92.4</span></td>
      <td><span>90.2</span></td>
      <td><span>88.2</span></td>
      <td><span>84.6</span></td>
      <td><span>76.2/83.1</span></td>
      <td><span>84.5</span></td>
      <td><span>86.3</span></td>
      <td><span>0.884</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="McCann et al. (2017), <i>Learned in Translation: Contextualized Word Vectors</i>">Char+CoVe-L</span></td>
      <td><span>en-de, WMT 2017</span></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td><span>95.8</span></td>
      <td><span>90.3</span></td>
      <td></td>
      <td><span>88.1</span></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Kiela et al. (2017), <i>Learning Visually Grounded Sentence Representations</i>">Cap2Both</span></td>
      <td><span>COCO</span></td>
      <td><span>79.6</span></td>
      <td><span>81.7</span></td>
      <td><span>93.4</span></td>
      <td><span>89.4</span></td>
      <td></td>
      <td><span>84.8</span></td>
      <td><span>72.7/82.5</span></td>
      <td><span>76.1</span></td>
      <td><span>81.6</span></td>
      <td></td>
    </tr>
    <tr>
      <td>
        <div><span class="span-full" data-tippy-content="Subramanian et al. (2018), <i>Learning General Purpose Distributed Sentence Representations via Large Scale Multi-task Learning</i>.">
            MTL</span>, +STN+Fr+De+NLI+2L+STP</div>
      </td>
      <td><span>Multiple</span></td>
      <td><span>82.8</span></td>
      <td><span>88.3</span></td>
      <td><span>94.0</span></td>
      <td><span>91.3</span></td>
      <td><span>92.6</span></td>
      <td><span>83.6</span></td>
      <td><span>77.4/83.3</span></td>
      <td></td>
      <td><span>87.6</span></td>
      <td><span>0.884</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Wieting et al. (2015), <i>Towards Universal Paraphrastic Sentence Embeddings</i>.">Paragram</span></td>
      <td><span>PPDB (XL)</span></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td><span>79.7</span></td>
      <td></td>
      <td></td>
      <td><span>84.94</span></td>
      <td><span>0.831</span></td>
    </tr>
    <tr style="border-top: 3px solid #000;">
      <td><span class="span-full" data-tippy-content="Logeswaran et al. (2018), <i>An efficient framework for learning sentence representations</i>.">MC-QT</span></td>
      <td><span>BookCorpus</span></td>
      <td><span>80.4</span></td>
      <td><span>85.2</span></td>
      <td><span>93.9</span></td>
      <td><span>89.4</span></td>
      <td><span>92.8</span></td>
      <td></td>
      <td><span>76.9/84.0</span></td>
      <td></td>
      <td></td>
      <td><span>0.868</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Hill et al. <i>Learning Distributed Representations of Sentences from Unlabelled Data</i>.">SDAE</span></td>
      <td><span>BookCorpus</span></td>
      <td><span>74.6</span></td>
      <td><span>78</span></td>
      <td><span>90.8</span></td>
      <td><span>86.9</span></td>
      <td><span>?</span></td>
      <td></td>
      <td><span>73.7/80.7</span></td>
      <td></td>
      <td></td>
      <td><span>0.46*</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Hill et al. <i>Learning Distributed Representations of Sentences from Unlabelled Data</i>.">FastSent</span></td>
      <td><span>BookCorpus</span></td>
      <td><span>70.8</span></td>
      <td><span>78.4</span></td>
      <td><span>88.7</span></td>
      <td><span>80.6</span></td>
      <td></td>
      <td></td>
      <td><span>72.2/80.3</span></td>
      <td></td>
      <td></td>
      <td><span>0.72*</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Kiros et al. (2015), <i>Skip-Thought Vectors</i>.">SkipThought</span></td>
      <td><span>BookCorpus</span></td>
      <td><span>76.5</span></td>
      <td><span>80.1</span></td>
      <td><span>93.6</span></td>
      <td><span>87.1</span></td>
      <td><span>92.2?</span></td>
      <td></td>
      <td><span>73.0/82.0</span></td>
      <td></td>
      <td></td>
      <td><span>0.858</span></td>
    </tr>
    <tr>
      <td>
        <div><span class="span-full" data-tippy-content="Gan et al. (2016), <i>Learning generic sentence representations using convolutional neural networks</i>.">Hierarchical+Composite-emb</span></div>
      </td>
      <td><span>BookCorpus</span></td>
      <td><span>77.77</span></td>
      <td><span>82.05</span></td>
      <td><span>93.63</span></td>
      <td><span>89.36</span></td>
      <td><span>92.6</span></td>
      <td></td>
      <td><span>76.45/83.76</span></td>
      <td></td>
      <td></td>
      <td><span>0.861</span></td>
    </tr>
    <tr>
      <td>
        <div><span class="span-full" data-tippy-content="Tang et al. (2018), <i>Exploiting Invertible Decoders for Unsupervised Sentence Representation Learning</i>.">Invertible linear projection</span></div>
      </td>
      <td><span>BookCorpus</span></td>
      <td><span>81.3</span></td>
      <td><span>83.5</span></td>
      <td><span>94.6</span></td>
      <td><span>89.5</span></td>
      <td><span>90</span></td>
      <td><span>85.9</span></td>
      <td><span>76.5/83.7</span></td>
      <td></td>
      <td><span>85.2</span></td>
      <td><span>88.1</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Radford et al. (2017), <i>Learning to Generate Reviews and Discovering Sentiment</i>.">byteLSTM</span></td>
      <td><span>Amazon reviews</span></td>
      <td><span>86.9</span></td>
      <td><span>91.4</span></td>
      <td><span>94.6</span></td>
      <td><span>88.5</span></td>
      <td></td>
      <td></td>
      <td><span>75.0/82.8</span></td>
      <td></td>
      <td></td>
      <td><span>0.792</span></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Cer et al. (2018), <i>Universal sentence encoder</i>.">Deep averaging network</span></td>
      <td>
        <div><span>Multiple</span></div>
      </td>
      <td><span>74.45</span></td>
      <td><span>80.97</span></td>
      <td><span>92.65</span></td>
      <td><span>85.38</span></td>
      <td><span>91.19</span></td>
      <td><span>77.62</span></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Cer et al. (2018), <i>Universal sentence encoder</i>.">Transformer</span></td>
      <td><span>Multiple</span></td>
      <td><span>81.44</span></td>
      <td><span>87.43</span></td>
      <td><span>93.87</span></td>
      <td><span>86.98</span></td>
      <td><span>92.51</span></td>
      <td><span>85.38</span></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td><span class="span-full" data-tippy-content="Yang et al. (2018), <i>Zero-training Sentence Embedding via Orthogonal Basis</i>.">GEM (+ LFP)</span></td>
      <td><span>--</span></td>
      <td><span>79.8</span></td>
      <td><span>82.5</span></td>
      <td><span>93.8</span></td>
      <td><span>89.9</span></td>
      <td><span>91.4</span></td>
      <td><span>84.7</span></td>
      <td><span>75.4/82.9</span></td>
      <td></td>
      <td><span>86.2</span></td>
      <td><span>86.5</span></td>
    </tr>
    <tr style="border-top: 3px solid #000;">
      <td><span class="span-full" data-tippy-content="Zhang et al. <i>Sentence-State LSTM for Text Representation</i>.">sLSTM</span></td>
      <td></td>
      <td><span>82.45</span></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>
        <div><span class="span-full" data-tippy-content="Lin et al. (2017), <i>A Structured Self-attentive Sentence Embedding</i>.">Structured self-attention</span></div>
      </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td><span>84.4</span></td>
      <td></td>
      <td></td>
    </tr>
    <tr>
      <td>
        <div><span class="span-full" data-tippy-content="Yoon et al. (2018), <i>Dynamic Self-Attention : Computing Attention over Words Dynamically for Sentence Embedding</i>.">Dynamic self-attention</span></div>
      </td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td></td>
      <td><span>88.5</span></td>
      <td></td>
      <td><span>87.4</span></td>
      <td></td>
      <td></td>
    </tr>
  </table>
</div>

<script>
  tippy('.span-full', {arrow: true, arrowType: 'round', size: 'large', interactive: true})
tippy('.span-init', {arrow: true, arrowType: 'round', size: 'large'})
</script>
