---
layout: post
title: ACL 2019 - a review 
comments: true
description: todo
excerpt_separator: <!--more-->
---

ACL 2019, held in Florence, Italy, was one of (the?) largest ACLs to date. The event was spread out over 8 halls, with simultaneous talks (up to 6 simultaneous tracks) and a poster session in every slot. ACL was co-located with several (almost too many) excellent workshops with a narrower focus than the main conference. In this post, we're going to attempt to condense the conference into a quick description of posters and talks that we thought were cool.

# Monday
The conference kicked off on a Monday (after a tutorial-filled Sunday) and was filled with delayed and cancelled flights, with flustered looking attendees arriving throughout the day, laden with massive suitcases.

### Multi-Task Semantic Dependency Parsing with Policy Gradient for Learning Easy-First Strategies
This paper is another success story for reinforcement learning in NLP. As more and more people try to use techniques like policy gradient, they realize it’s not as easy as it seems to get good results (see: https://arxiv.org/abs/1907.01752). But for semantic dependency parsing, giving the parser a very free hand in choosing attachment order and learning it with RL seems to work quite well and lead to an easy-first strategy. 

# Tuesday

The keynote by Liang Huang on Simultaneous Interpretation was rather inspiring, showing that a very simple idea (prefix-to-prefix training for MT) can be very effective in allowing a system to learn predictive translation automatically. This seems like something many groups have been working on recently, and Baidu demonstrated its success by deploying such a system. Too bad for the 3000 worldwide certified simultaneous interpreters’ jobs...

### We need to talk about standard splits 
This paper is a very welcome critique of the accepted wisdom of standard splits, i.e. the principle of consistently using the same training/validation/test split to evaluate different systems. Their critique suggests that the use of a single standard split may result in Type I errors. 

### Is attention interpretable? 
A very comprehensive work on a theme that’s been growing popular lately - Serrano et. al discuss the value of self-attention as a tool for visualising importance, by measuring when output decision flips occur based on the blanking-out of “more attentive” words, and concludes that it isn’t as interpretable as we’d assumed it was. An upcoming emnlp paper presents counter-arguments: https://arxiv.org/abs/1908.04626

### Correlating neural and symbolic representations of language
Representational similarity analysis, or RSA, was a recurring topic this ACL. Chrupala & Alishahi present correlating neural and symbolic representations of language, where they extend traditional RSA -- which relies on comparing representations from discrete media -- to what they call RSA_regress, where the predictability of one representation from the other is analysed.

### Energy and policy considerations
A paper that’s made NLP collectively take a long, hard look at itself is Strubell et al.’s energy and policy considerations, which describes exactly how much CO2 we’ve been emitting when we train our models. The numbers, when you add them all up, are quite alarming, and bear thinking about. Can experiments be worthwhile? 

### On the distribution of deep clausal embeddings
Thematically slightly different to a lot of papers this ACL, “On the distribution of deep clausal embeddings” presents an analysis of large, automatically annotated corpora to verify phenomena discussed in theoretical linguistics -- specifically, whether there are hard constraints on the depth of clausal embeddings. They discover that while deeply embedded clauses tend to be shorter, there’s no actual hard constraints on embedding depth.

### Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference 
This paper is a nice and clear demonstration that current NLI test sets are not testing what we think they are testing. Together with the Standard Splits paper and many other papers on artefacts and evaluation, the community is hopefully starting to realize we need to be a lot more rigorous about test sets and not just hope that sampling from the general distribution will teach us something useful. Even if most problems in life are easy to solve, it doesn’t mean we should overfit to them and ignore the relatively few hard cases!

# Wednesday

### Bridging the Gap between Training and Inference for Neural Machine Translation
Our discussion on Wednesday kicks off with the best long paper this ACL. The idea behind the paper is as ingenious as it is simple - NMT systems are conventionally trained with ground truth previous words during decoding. This is obviously not replicable at test time, and the authors propose methods to bridge this gap, by sampling from the predicted (“oracle”) distribution. They propose methods that sample from world-level distributions, and compare these with a method of sampling complete sentences - using beam search, and BLEU as a metric - and using the words from the entire sampled sentence as context. 

### BERT rediscovers the classical NLP pipeline
This paper had been picking up some buzz on Twitter before ACL. It’s a great addition to the recent theme of probing pretrained sentence encoders. The authors tack diagnostic classifiers onto scalar mixes of BERT layers and attempt to recover various levels of linguistic annotation; they discover that, layer-by-layer, BERT tends to resemble the classic NLP pipeline, with lower layers likelier to contribute to eg. POS tags, whilst higher layers contribute more to structured, “complex” tasks like NER.

### A surprisingly robust trick for WSC 
The authors describe how the Winograd schema challenge - a corpus with “commonsense reasoning” based pronominal disambiguation tasks - can be made a lot easier by fine-tuning on other pronoun resolution datasets, which is surprising, to quote the authors, “because previous work (Opitz and Frank, 2018) implies that generalizing to WSC273 is hard.” 

### What kind of language is hard to language model? 
The paper asks what turns out to be quite an elusive question. When trying to design better language models we come across different challenges in different languages, but how do you measure language modelling difficulty objectively? A follow up on last year’s Are All Languages Equally Hard to Language-Model?, here they even experiment with non-European languages too! Carefully controlling for superficial factors and biases in the data, they find that contrary to previous results, linguistic factors have no strong correlation with LM difficulty, but vocabulary size and sentence length do.

### How multilingual is multilingual BERT?
An extension to the let’s-probe-BERT series of papers into a multilingual domain. The authors use task-specific annotations in one language to fine-tune BERT, and evaluate the fine-tuned model on different languages. The trained models do work after transfer, even on languages with completely different scripts (and therefore zero lexical overlap), indicating that multilingual BERT is pretty multilingual. 

### Analyzing multi-headed self-attention
The transformer is, in general, widely seen as a fundamental building block for NLP, particularly for tasks like machine translation. Multi-headed self-attention (MHSA) is a critical component of the transformer architecture. In Analyzing MHSA, the authors evaluate per-head contribution, and discover that the “most confident” heads play consistent 

### Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation 
The authors tackle a tough issue in semantic parsing, namely, that SQL sucks. By converting it to a friendlier representation for parsers, they show a significant improvement on the recent Spider dataset, which seems to be orthogonal to other recent contributions such as Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing.

### Compositional Semantic Parsing across Graphbanks 
A nice demonstration that you can learn a general semantic parser that’s better on many representations, rather than focusing on one and treating the others as “auxiliary” (as done previously in Multitask Parsing Across Semantic Representations). This is an attractive teaser for the CoNLL 2019 shared task on Cross-Framework Meaning Representation Parsing.

# BlackboxNLP 2019

After 2018's stellar reception, BlackboxNLP was renewed for its second chapter this year. 

### Transcoding compositionality
In transcoding compositionality, the authors present seq2attn, an architecture designed to discover compositionality - the two main innovations that induce a bias towards compositionality involve expressing the attention as a weighted sum of non-contextual embeddings rather than of contextualised vector, and using a learned initialisation vector for the decoder, instead of the encoder's last state - this encourages the decoder to examine "disentangled" representations of the input. They evaluate their approach on two composition-oriented datasets, and also test the ability of seq2attn to overgeneralise given non-compositional examples.

### Blackbox meets blackbox
Another RSA-themed paper, the authors propose an extension to RSA that they dub “ReStA” - representational stability analysis. The motivation is to compare, instead of two separate encoders, the same encoder, given a change in one particular parameter. The authors perform two sets of experiments: in the first, they compare encoders to each other, varying the length of prior context (the task being modelling a Harry Potter corpus); in the second, they use RSA to compare the encoders to fMRI data obtained from participants reading the same text. Their observations show strong differences between language models for the first part, and that (amongst other observations) LSTM based models tend to be more similar to fMRI readouts.

### Character eyes
“Character Eyes” attempts to “see language through character-level taggers”; the authors introduce a metric called the POS-discrimination index, which is higher for a hidden unit if it acts as a better discriminator for POS tagging. They observe imbalances across forward and backward activations, particularly for agglutinative languages, where the taggers tend to prefer the more imbalanced models. They also show that prefixing languages tend to be more imbalanced in favour of backward units, with the opposite true for suffixing languages, suggesting that “LSTMs are better at detecting formations towards their final state”.

### What does bert look at
And finally, the winner of the best paper award at the workshop - this paper is an analysis, not of BERT's hidden representations, like most other probing work, but of its attention heads. The authors extract valuable linguistic insight, including some surprising results, such as the fact that heads within the same layer have similar attention distributions.
Compositionality

# DMR (Designing Meaning Representations) workshop
Superficiality of design decisions should be clearly defined and abstracted away
The elusive distinction between form and meaning is an engineering challenge
Important to learn best practices for universal representation that’s still informative

Historical language change
The meaning aspect of language is hard to define, and meaning change is even harder
Exact mathematics and careful statistics are essential to develop this field

Gender Bias
Lipstick on a pig

